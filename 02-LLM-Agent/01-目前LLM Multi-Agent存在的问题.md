1. 历史对话过长，无法压缩历史对话

2. 将问题划分为子问题时，容易出现划分子问题过多，最后无法整合出一个可行的方案

3. 对人类用户的意图感知不足，在用户提出一个比较粗糙的问题时，LLM可能不会沿着正确的方向前进。在哪个地方需要人类反馈进行改进，是个值得思考的问题。

4. LLM 擅长的是经验，而不是规划。规划问题重要的是要给出的回答要在该场景下“有用”，并能控制方案的总成本。规划能力的学习要么靠外部指导求解过程，要么需要类似RL的学习环境。

5. 外部注入经验：主动学习规划能力目前在成本上是不可行的。那么想要提升规划能力，就需要外部注入求解过程的经验。外部经验可以来自于：
    - 从LLM自己的世界知识中提取
    - Agent框架本身的流程和机制设计
    - 随任务一起输入的建议求解方式
    - Agent包含的历史解决方案库

6. 在目前MultiAgent框架中，还需要的一类功能是LLM在信息不完备时候的主动提问功能。如果前一级输入的信息不完备，就会导致后面的结果都有问题。这导致了现在的框架更加不能丢弃完整的message history，或贸然对其进行压缩。