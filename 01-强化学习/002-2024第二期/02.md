数据流调度、视频传输、路由选择、TCP拥塞控制以及缓存问

- 2.1 任务及数据流调度
    - 2.1.1 面向独立任务的调度
        - MIT 的工作 DeepRM［19］
        将DRL运用在数据中心调度中。DeepRM观测系统状态，包括 CPU 和内存的占用情况以及任务队列中任务的资源请求状况。DeepRM的决策即调度当前任务队列中一个或多个任务到CPU或待执行队列中。奖励函数为 ∑ -1/T，T代表每个任务的请求时间。
        - DeepJS［21］
        优化目标是最小化Makespan，其中Makespan定义为最后一个任务的完成时间。DeepJS基于神经网络计算不同的( 任务，机器) 数据对的适应度( Fitness) ，在分配任务时，将任务分配到适应度最高的机器。
    - 2.1.2 面向关联任务的调度
        - Spear［24］：结合蒙特卡洛树搜索(MCTS)和DRL技术实现对关联任务的调度
        DRL辅助MCTS进行搜索选择。
        任务之间的关联关系被建模为有向无环图(DAG)，DRL智能体的输入状态包括系统资源的占用状况、队列中有限任务个数对资源的请求状况、每个节点的孩子节点个数、有关运行时间以及任务负载的关键路径信息，输出动作为选择一个队列中的任务去调度或者空动作引发时间前进一步。
        - Mao 等人提出的 Decima［25］：也是一种DAG和DRL的结合，再加入图神经网络。
        - 文献［27］：在异构服务器的场景下提出了基于多任务深度强化学习(Multi-taskLearning)的调度算法进行高效调度并行任务
    - 2.1.3 面向计算和节能的联合调度：合理调度实现计算能耗和冷却能耗的高效利用
        - DeepEE［29］
        策略网络输出调整气流速率的连续动作，该连续动作也将作为 DQN中Q网络( Q－network) 输入状态的一部分，依据Q-network的输出可确定将任务调度到哪台服务器执行的离散动作
        - Deep－EAS［30］则是针对异构服务器利用DRL技术进行高效计算和节约能量的联合优化
        - 文献［31］提出使用分层架构去解决任务调度和能量管理的问题，包括全局层面和本地层面。
    - 2.1.4 面向数据中心网络流的调度
        - 针对单独的数据流，Auto［35］利用DRL技术进行自动化的数据中心网络流调度。
        Auto采用了具有外围系统和中心系统的二级架构。外围系统直接部署在终端服务器，采用多级队列的方式进行短流本地快速决策。而判断数据流优先级的阈值则每隔一段时间由中心系统的DRL智能体(SRLA)决定。中心系统还有另一个智能体(LRLA)负责只针对长流的决策，包括确定优先级、速率和路径。

- 2.2 视频传输
    - 2.2.1 视频点播
        - Gadaleta 等人［43］提出用基于 DQN 的机器学习框架，通过神经网络近似动作值函数
        - Mao 等人于 2017 年提出 Pensieve［44］，该模型通过客户端视频播放器获取过去 8 个历史时刻信息，Pensieve 基于系统状态选择下一个视频块的请求码率。
        - 为了将 Pensieve 在实时运行过程中的计算开销降低,Huang 等人将 DＲL 与传统算法 BBA 进行结合，提出了 Stick 网络［45］。Stick 将原本 Pensieve 的离散动作修改为连续动作，即 BBA 决策的上下边界。客户端通过修改后 BBA 算法的上下边界和当前缓存长度去选择请求的下一个视频块码率。
        - Huang 等人同时又提出一个轻量级的 Trigger 网络，用来决策当前情况是否需要调用 Stick 网络来对 BBA 的上下边界进行调整，以减少频繁使用 Stick 算法带来的计算开销
    - 2.2.2 360全景视频传输
        - Zhang 等人［47］于 2019 年提出结合 LSTM 神经网络的 Actor －Critic 模型的 DＲL360 流媒体框架。
    - 2.2.3 直播

- 2.3 路由 : 数据包路由和域内流量规划
    - 2.3.1 数据包路由
        - Boyan 等人［51］首次将强化学习应用于数据包路由问题中，提出了动态的路由转发策略 Q-routing算法.
        - Choi 等人［52］ 提出了PQ-routing算法，利用历史的路由信息来预测链路流量; Kumar 等人［53］提出了 DＲQ-routing 算法，利用前向和后向探索信息来学习更优的路由决策。
        - Mukhutdinov 等人［56］基于多智能体 DQN 算法.在多智能体强化学习中，各路由器被视为独立的智能体，且拥有独立的神经网络用于路由决策。各智能体仅可观测到局部状态信息.各智能体的神经网络参数全局共享。
        - You 等人［57］将完全分布式多智能体强化学习应用于数据包路由问题中，以最小化数据包平均传输时延为目标，设计了端到端的动态数据包路由算法DQRC。各路由器视为独立的智能体，均拥有独特的神经网络用于参数更新和动作选择，且其学习过程和决策过程均为分布式。DQRC算法加入了历史决策、队列中未来数据包目的节点以及相邻节点队列长度等状态信息。
        - Ali 等人［60］基于 DDQN 强化学习算法［61］，设计了分层数据包路由算法 DDQN-routing，以平衡网络链路负载。
    - 2.3.2 域内流量规划
        - Xu等人［62］将DRL应用于域内流量工程(Traffic Engineering，TE) 问题中，并提出了经验驱动的域内流量工程方案DRL-TE。
        - Stampa 等人［54］ 将 DＲL 应用于软件定义网络(SDN)。基于单智能体强化学习算法 DDPG［15］，SDN-routing 算 法 将SDN控制器视为集中式智能体，该智能体能够观测网络的全局信息，并控制网络中各路由器的数据流分配决策。
        - Yu 等人［64］基于 DDPG 算法设计了通用的SDN路由优化算法DROM。
        - Sun等人［66］结合RNN和DDPG算法，设计SDN 路由优化算法 TIDE，以处理具有时间序列特性的网络状态。

- 2.4 TCP拥塞控制
    - 2.4.1 面向TCP整体性能的拥塞控制
        - Li 等人［71］在 2018 年提出了基于 Q-learning 的自适应拥塞控制算法 QTCP。这些状态分别是发送包的平均间隔时间、接收到 ACK 包的平均间隔时间和平均RTT。奖励函数U = a·log(throughput) － b·log(RTT)
        - Yan 等人［72］在2018年提出了基于模仿学习的拥塞控制算法Indigo。
        - Jay 等人［73］在 2019 年提出了基于DRL的拥塞控制算法 Aurora。Aurora 的输入状态包括延迟梯度、延迟比和发送比率。Aurora 把神经网络的输入状态设置为当前时刻之前 10 个时间片的状态组合，从而获取更充分的历史信息。
    - 2.4.2 针对特定传输问题的拥塞控制
        - 在 2019 年提出 TCP-RL［74］来动态地选择初始拥塞窗口。TCP-RL 把初始拥塞窗口的选择看做一个多臂老虎机问题，并用折扣 UCB 算法对其进行训练。而在速率调节阶段，TCP-RL并未使用DRL的神经网络来直接产生调节动作，而是使用神经网络的输出来选择现存的14 个拥塞控制算法之一，作为当前时间段的调节算法。
    - 2.4.3 与传统算法相结合的拥塞控制
        - Abbasloo 等人提出 DeepCC［80］，用DRL给传统拥塞算法的调解结果制定一个最大值，来优化传统算法。
        - Emara 等人于 2020 年提出的 Eagle［81］，借助BBR算法的专家知识来优化训练的 DRL拥塞控制算法。
        - Abbasloo 等人提出了基于Cubic和DRL的混合算法［82］。DＲL 在每个固定时间周期中收集信息，然后计算出一个基础拥塞控制窗口，之后在下一个时间周期中，Cubic 基于这个拥塞控制窗口数值根据自身的控制逻辑进行调节。

- 2.5 网络缓存
    - DeepCache［84］的模型包括两部分: 基于LSTM编码器模型的目标特征预测器和缓存替换管理模块。其目标在于提前预测出到达请求的目标特征，自动学习到达流量模式的变化。
    - RL-Cache［85］研究采用 DＲL 设计缓存接纳策略，即当一个没有被缓存的请求到达时，是否要将该请求内容接纳至缓存中，缓存的剔除策略依旧采用LRU。
    - Kirilin 等人［86］提出使用前馈神经网络预测内容流行度，并运用最小堆去实现LFU的缓存替换策略。
    - Wang 等人［87］提出采用纯强化学习设计缓存算法。输入状态为一个四元组: 到达请求的大小、上次该内容被请求的时间间隔、该目标迄今为止的访问次数和剩余的缓存大小比例。所采用的训练方法为标准的 A2C 强化学习算法。强化学习的输出动作为是否接纳一个未在缓存的新请求。奖励为自上次决策以来总的命中字节数。


        
