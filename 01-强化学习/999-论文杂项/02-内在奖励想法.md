- 如何定义一个状态的信息量
    - St+1 和 St 相比，信息量提升了多少

- 局部重要性观测：
    - P(At | St, St`)
    - St`: 局部观测状态。 St``还可以时部分历史信息
    - 如何确定局部观测状态St`：
        - 计算局部St`对于St的重要性权重
    - 内在奖励：
        - 在挑选局部状态的时候，可以将重要性权重作为顺带的内在奖励

- 显式告知智能体有关内在奖励的信息
    - 目前的内在奖励探索，普遍都是只给一个奖励值，并不提供产生这个奖励的详细信息
    - 通过某种特征工程的方法，将内在奖励的信息转换为一个向量，然后与状态向量组合

- RND 的改进：
    - 参考 02-13 文章，在 RND 的先验模型中加入一个额外的层(FiLM)，来扩展输入的特征
    - 将 f(s) 修改为 f(s, pai(a|s))，在最大化 RND 的过程中，顺便优化一下策略

- 智能体在某些区域徘徊的原因： 要让内在奖励尽量实现外部奖励最大化
    - 在多变的环境中，判断新颖性的方法难以预测未来的变化，因此智能体可能会被困在这个区域
        - 电视机噪音问题
    - 基于计数的奖励，容易让智能体在局部最小值的区域内徘徊
    - 即便已经充分探索过某些区域，但是内在奖励函数还是会提供非零奖励
        - 如何让奖励变得更为稀疏

- 对现在所有内在奖励的通用改进方法    
    - 根据02-14中描述，情景奖励优于生命周期奖励：
        - 可以添加一种重置的机制，从生命周期奖励转变为情景奖励
        - 训练一个终止器，让它选择什么时候停止采用内在奖励，什么时候启用内在奖励
    - 能否提前计算下一步的内在奖励
        - 设计一个预测器，它独立于内在奖励模型，预测下一个状态，通过下一个状态计算下一步的内在奖励

- 目前很多程序生成环境中，都默认环境是可以被计数的，就是状态不是无限的。
    - 可以提出一种方法，打破这种对环境状态有限性的假设
    - 02-14 02-16 都希望奖励更加稀疏，但是需要状态计数的支持。但这很大程度局限了这种方法的实用性。
    - 可以在原先的实验中，把内在奖励推迟几个时间步，看看效果是不是还是一样
        - 这样做的目的：因为采用上面停止器的方法，可能无法准确在合适的时间给出奖励，不确实停止器什么时候工作
        - 即便这一回合没有进行内在奖励计算，也要训练RND，要不然容易出现每次计算内在奖励都是好的分数

- 如何解决电视机噪音问题
    - 如何让内在奖励的计算都集中于有用的状态信息