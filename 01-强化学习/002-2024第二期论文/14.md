14-Revisiting Curiosity for Exploration in Procedurally Generated Environments


##### 核心观点：
- 在程序生成环境中，仅使用情景内在奖励即可促进探索，效果超过了终身内在奖励

- 终生内在奖励为什么无效：
    - 终生内在奖励无法准确的表示环境的新颖性
    - 实验设置：
        - 将每组{at,st,st+1}的内在奖励rt，进行随机组合
        - 实验发现，即便每步的内在奖励和它这一步的状态无关，但是也并不会对效果有重大影响

- 情景内在奖励：
    - 情景内在奖励是针对每一个单独的情节提供的奖励，目的是激励智能体在该情节内探索新的状态。
    - 比如，如果智能体在一次游戏回合中首次访问了一个位置，它会得到情景内在奖励。
- 终生内在奖励：
    - 终身内在奖励则是跨越智能体生命周期内所有情节的累积经验提供的奖励，用以激励智能体访问长期内较少出现的新状态。
    - ICM、RND 都是终生内在奖励

##### 论文2.2节：
- ICM 以及 ICM 改进型 RIDE
- RND 以及 RND 改进型 BeBold
- 计数奖励：Ride

##### Mini-Grid Benchmark: 
- Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.