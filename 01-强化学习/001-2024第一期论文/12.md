12-SAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKING THE REPLAY RATIO BARRIER

总结：这篇文章采用重返率缩放，以及网络参数重置，来提高RL数据效率

##### 主要方法：重放率缩放、参数重置
- 重放率/重播率 Repay Ratio ： 参数更新次数/与环境交互收集数据次数
    - 主要是提高重放率，例如训练16次，再进行数据收集
- 参数重置
    - 在固定训练次数后，重置部分网络参数，或者全部重置，或者采用权重α重置



##### 其他信息：

- 由于过多的训练，会导致模型的泛化能力下降，因此（The primacy bias in deep reinforcement learning. In ICML, 2022.）提出了定期重置网络参数

- 设置较高的重放率，这类似于Offline RL。但是通过实验发现，即便在重置参数后，大部分Repay Buffer中的数据不是当前代理的互动数据，但是后续少部分的互动数据对代理的训练至关重要。
    - 为什么Repay Buffer中的数据不是当前代理的：因为Agent参数被重置了，因此可以这么认为
    - 为什么后续的数据是少部分的：因为重放率很高，代理与环境交互的次数有限
    - 如何证明至关重要：5.1.2节
    - 图5展示了，如果配上Offline RL的方法，效果会更好


- 局限性：重放率
    - 需要保存所有的历史数据
    - 会导致训练时间变长

- 结论：
    - 通过利用代理参数的部分或全部重置，可以解锁有利的重播率缩放的新水平，从而提高无模型深度强化学习算法的样本效率
