Goal-conditioned Offline Planning from Curious Exploration

- 1 这篇论文首先指出目前强化学习在一些应用下存在局部最优的情况 $T-loacal-optima$

- 2 针对这种情况，作者提出了两种方法
    - 2.1 基于模型的规划方法（Model-based Planning），通过对未来状态的规划来选择动作。这个动作序列长度称为 horizen H，当H大于局部最优的深度时，模型就可以逃离局部最优。这种方法能够缓解局部最优问题。
    - 2.2 基于图的聚合(6.2节)，这个方法已经有人做过了(B. Eysenbach等, Search on the replay buffer: Bridging planning and reinforcement learning. NIPS 2019)。他在这个方法上面继续做了一些改进。具体改进细节在附件D。


这篇文章后续要看的内容:
- 1 Goal-Conditioned Reinforcement Learning
- 2 Unsupervised Exploration 
- 3 Graph-based Value Estimation
