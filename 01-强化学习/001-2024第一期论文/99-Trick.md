01：GAN充当奖励，从专家数据中学习奖励

02：对于RL算法的输入，重新添加一个历史信息，这个历史信息是用AutoEncoder模型学出来的

03：采用规划和图聚合的方法，帮助模型避免陷入局部最优

04

05：为强化学习设计一个新的类似RNN的模块

06：Transformer能够增强RL算法的记忆能力，但并没有改善长期信用分配的能力

07：基于影响力和好奇心的探索策略

08：好奇心 ICM

09：多智能体中的影响力 MOA

10：将MCTS算法运用在DRL之中

11：基于10改进而来，添加了一些深度学习方法

12：采用重返率缩放，以及网络参数重置，来提高RL数据效率

13：为一个任务训练多个智能体，然后采用某种算法挑选或合并多个模型的决策

14：缓冲区，优先级算法

15：训练的过程中重置部分网络参数

16：定义多种内在奖励方法，每次训练选择其中一个

17：外层有一个策略Ω选择到底用哪一个决策策略Π生成动作，内层还有各一个算法，用于选择Π内的动作

18：内在奖励，RND
