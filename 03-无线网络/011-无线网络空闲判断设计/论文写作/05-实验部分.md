#### Experiment Results
本章节将评估DM-MDN预测的准确性验证和其实际性能，其次，对比不同LLM-MAS方法的性能及其在资源占用方面的差异。

##### Experimental Setup
###### Simulation platform
在本研究中，我们构建的仿真环境基于IEEE 802.11g标准，采用2.4 GHz频段，传输速率为54 Mbit/s。在信号传播方面，则采用自由空间路径损耗（FSPL, Free Space Path Loss）模型来描述信号功率随距离的衰减情况。
网络拓扑为星型拓扑，由单个接入点和3至7个客户端节点组成，节点数量根据实验场景需求进行调整。在Dynamic Traffic Model 章节中提到的三元组参数流量模型。以下为三个参数均匀分布采样的范围：offset(0,3000us)：起始偏移时间，interval(500,2500us):数据包到达间隔，prob(0.7,0.99):数据包到达概率。对于数据包优先级的deadline做以下设置：优先级1的数据包不做deadline限制，优先级2的数据包deadline为1200us，优先级3的数据包deadline为900us。后续实验的数据包丢包率计算仅考虑数据包最终未能在deadline前交付的情况，途中因碰撞等原因导致的发送失败不计入丢包率。公式（2）中的预测间隔k和公式（5）中的a，分别设置为1和0.1。
###### 模型结构
DM-MDN模型由三层神经网络构成主干，配合四个单层神经网络分支作为输出结构，全部使用ReLU激活函数。为进行性能评估，我们选择了两种预测对比模型：一种基于GRU，由PyTorch标准GRU结构堆叠4层并接两层神经网络输出层；另一种基于Transformer，其编码器采用PyTorch提供的TransformerEncoderLayer堆叠2层，输出层为单层神经网络。


##### 预测实验结果
###### 预测精度结果
DM-MDN的本质是对网络负载进行识别并给出在当前负载下信道空闲时隙长度的区间。在无线网络中，信道空闲时隙长度具有高度随机性，因此DM-MDN在特定均值和标准差下的高斯采样，其预测值会有很大的误差。然而，DM-MDN的优势在于其能够准确识别网络负载状态，并据此动态调整采样区间。正是基于这一特性，DM-MDN在预测精度上相比GRU和Transformer模型展现出显著优势。
表 1 展示了 DM-MDN 和 MDN 在随机采样下相较于对比模型在测试数据集上的预测Mean Absolute Error(MAE)对比结果。测试数据集包含了不同网络负载条件下的数据，并且网络负载变化是完全随机特性。需要特别说明的是，在该实验中，DM-MDN 的均值计算公式为 μ = μ_base + μ_offset，这主要是因为我们旨在验证 DM-MDN 的预测精度而不是实际表现。从表 1 数据可以看出，DM-MDN 在预测精度方面较 GRU 和 Transformer 分别领先了约 xx% 和 xx%，展现了其在复杂网络环境下的优势。此外，DM-MDN 与 MDN 在预测精度上差异极小，这侧面证明了将均值进行结构化分解并不会损害模型的整体预测能力。

###### 预测模型在仿真中的结果
我们在三种不同负载场景和三种流量模型下，比较了DM-MDN、Transformer、GRU和CSMA/CA在网络仿真中的性能表现。表2展示了这四种方法在数据包平均时延、丢包率和数据包吞吐量方面的对比结果。三个流量模型的参数分别为：Flow1：(2222，2107，0.96)，Flow2：(2741, 637，0.79)，Flow3：(930，1188，0.81)。其中，Flow1的数据包到达周期最长，但其数据包到达概率最高；Flow2的数据包到达周期最短，但数据包到达概率最大；Flow3则介于两者之间。我们特意选取了这三个具有鲜明特点的流量模型，目的是为了更加全面地评估各方法在不同场景下的性能表现。
在该实验中，MD-MDN、Transformer和GRU这三组实验，采用预测算法的节点数据包优先级为3，其他节点数据包优先级为1采用CSMA/CA接入算法。对于CSMA/CA实验，网络中所有节点均采用CSMA/CA接入算法，但是采用特定FLow的节点数据包优先级为3，其他节点数据包优先级为1。三种负载情况对于的发送数据包节点个数为：3(Low)、5(Medium)、7(High)。

xxxx（具体表现）
根据表2的实验结果。对于CSMA/CA算法，它的性能随着网络负载的增加而下降，特别是在网络负载较高的情况下，其性能明显退化。在网络中、高负载情况下，DM-MDN方法在丢包率、数据包平均时延和数据包吞吐量上均领先于其他对比方法。这表明DM-MDN方法能够在复杂网络环境下有效满足高优先级业务的实时性需求。值得注意的是，基于GRU和Transformer的预测算法在低负载条件下表现出显著的性能退化。这一现象与直觉相反，通常在低负载下，信道接入方法的性能应该是最优的。通过分析实验数据，我们发现这两种方法未能准确识别低负载环境，预测值仍然偏向高负载时的信道空闲状态。因此，它们的预测值较小，导致节点数据包在预测的空闲时隙内到达的概率较低，从而错失了最佳发送时机，这一情况与Fig 3中的场景3相符。相较之下，DM-MDN算法能够准确识别网络的负载情况，并根据网络的负载情况做出合理的信道空闲预测，因此其算法性能退化并不明显，整体保持相对稳定。

###### 不同分位数下的算法表现
对于DM-MDN而言，不同的分位数参数代表了不同的信道接入能力。理论上，分位数越高信道接入能力相对越强。但是由于网络场景的复杂性，最优分位数参数会随网络环境变化而不同。图3展示了在三种不同场景下，采用DM-MDN节点在不同分位数下的数据包平均丢包率表现。实验环境由6个节点构成，其中场景(a)、(b)、(c)分别对应1个、2个、3个节点采用DM-MDN作为信道接入算法的情况（采用DM-MDN的节点数据包优先级均为3），其他节点采用CSMA/CA。从图3可以看出，DM-MDN算法的丢包率整体呈现随分位数增加而降低的趋势。此外，随着网络中采用DM-MDN算法节点数量的增加，其丢包率也相应提高。值得注意的是，虽然存在一般性趋势，但最优分位数在不同场景下表现出明显的差异性。因此，为确定特定场景下的最优分位数参数，需要综合考虑历史决策经验和当前网络环境特征。


##### LLM-Agent 实验结果
###### 场景设计
为全面评估NetCogAgents资源分配性能，我们设计了具有动态优先级特性的无线网络仿真场景，包含5个节点和1个AP。每完成固定仿真时间周期后，部分节点数据包优先级随机变化，系统将更新后的优先级信息提交至资源分配算法，该算法根据节点实时性需求选择合适的信道接入方法（CSMA/CA或DM-MDN）并为DM-MDN设置分位数参数。随后系统执行新一轮仿真，循环往复，仿真变化轮次总数也由随机算法决定。考虑到实时网络环境中数据包可靠传输的重要性，我们主要采用高优先级节点丢包率作为评估指标，以验证算法的适应性和有效性。

<!-- 从图3可以看出，当网络中采用DM-MDN的节点达到总节点数一半时，其丢包率相较于其他场景有大幅波动。为了更加准确测试出分配算法对于网络场景变化的敏感度，在该实验中，采用高优先级的节点不会超过总节点数的一半。 -->

###### LLM-Agent 实验结果分析
我们将本文提出的NetCogAgents与Reflexion、StrategyLLM以及规则算法(贪心+自适应)进行了性能对比，其中Reflexion、StrategyLLM也是属于LLM-based Multi-Agent Systems方法。对于LLM-MAS方法，我们选取了四种LLMs，并在每种LLMs进行了三轮节点需求动态变化实验。表3展示了这四种方法在实验中高优先级节点的平均数据包丢包率。在这十二次实验中，每次实验节点的数据包优先级变化序列均不相同。
实验结果表明，NetCogAgents在平均丢包率方面优于其他方法，表现相对稳定，并没有出现丢包率非常高的情况。这主要归功于其高效的经验组织方式和反思机制所实现的快速适应能力。
Reflexion的性能波动最为显著，既有优异表现也有明显劣势。这种不稳定性主要源于其保留所有反思经验，随着系统持续运行，大量的反思经验导致其决策模块难以从中提取有效信息。StrategyLLM展现出较为稳定的性能，但其决策质量在很大程度上取决于初始生成策略的质量。在正确策略指导下，StrategyLLM能够接近最优决策；然而，若初始策略不当，其决策效果则显著降低。
规则算法主要采用贪心与自适应相结合的策略。具体而言，对高优先级节点应用DM-MDN方法，而低优先级节点则采用CSMA/CA方法。DM-MDN的分位数初始值设定为0.5，并通过自适应策略根据每次的丢包率结果以0.1为步长进行上下调整，以实现性能优化。这种贪心机制能让该方法在初期取得相对好的成绩，但是其策略调整较为机械，后续策略调整的速度明显不如其他方法。

###### 内存资源开销分析
考虑到网络管理设备的存储空间通常是有限的，同时，过长的决策历史输入也可能影响大型语言模型的推理效率与决策质量。因此，在设计NetCogAgents结构的时候就考虑到，需要在保证决策效果的前提下，尽可能地减少决策历史记录所占用的内存。图X展示了LLM-MAS方法在网络决策的过程中它们占用内存的情况。

在此需要说明，图中的"Memory Usage"并非指LLMs输出的实际内存占用情况。由于大语言模型的输出长度在不同的网络场景下会发生变化，即使在相同语境下也可能存在显著差异。因此，采用了更便于量化的统一计量方式：对于NetCogAgents，每条经验记录计为1单位内存占用；对于Reflexion，每条反思经验计为1单位内存占用；对于StrategyLLM，每条初始生成的策略计为1单位内存占用。

从图X中可以明显观察到Reflexion随着决策轮次增长内存占用也持续上升，这是由于网络一轮决策即可得到最终丢包率结果，因此每轮决策后都会生成新的反思内容。相比之下，StrategyLLM仅在系统初始化时生成预设的M条策略，后续若有策略更新也会直接覆盖原有策略，因此其内存占用始终保持稳定。NetCogAgents的内存使用则呈现出小范围波动但整体较低的趋势，这主要得益于反思智能体持续优化并精简其经验区内容，在反思智能体和特定的经验区管理规则共同作用下，NetCogAgents在内存占用表现上，均领先对比方法。

##### 消融实验分析
Fig x 展示了NetCogAgents在去除Reflection Agent和同时去除Reflection 和 Experience Agent的性能表现。在去除Reflection Agent后，经验区的经验数量将会呈现线性上升，而在同时去除Reflection 和 Experience Agent之后，将仅保留一个Decision Agent，它只能根据Prompt中已有的信息进行决策。NetCogAgents对于不同的LLMs表现出了不同的结果。对于Claude 3.5 和 Deepseek V3，在实验中它们的性能呈现出渐进式降低，每删除一个Agent就下降一些。而在Grok 3 和 ChatGPT 4o中，去除Reflection Agent对方法的性能的影响比同时去除Reflection 和 Experience Agent更大。这说明经验的累积对LLM-MAS的决策固然重要，但对长上下文理解能力较差的模型来说，对经验区的精简和管理更加重要。
