#### Experiment Results
本章节将从预测模型的准确性验证与实际网络性能评估两个方面展开实验分析，随后对比不同LLM Agent方法的性能及其在资源占用方面的差异。本实验基于一个开源的IEEE 802.11g网络仿真平台展开，该平台由Micheletto等人开发[###]。在所有实验中，我们采用了统一的网络协议栈配置和仿真参数（包括802.11物理层速率、MAC层退避策略等），并通过调整负载规模和节点需求等变量，针对性地评估各方法在不同场景下的表现。

##### Experimental Setup
###### Simulation platform
在本研究中，我们构建的仿真环境基于IEEE 802.11g标准，采用2.4 GHz频段，传输速率为54 Mbit/s。MAC层参数中，竞争窗口设置为CW_MIN = 16和CW_MAX = 1024，同时规定MAC帧格式中包头最大长度为34B、数据载荷最大长度为2312B；在信号传播方面，则采用自由空间路径损耗（FSPL, Free Space Path Loss）模型来描述信号功率随距离的衰减情况，从而更真实地反映无线传输的物理特性。
###### 模型结构
在预测实验中，采用了基于70000数据包仿真后获得的信道空闲数据作为训练样本，对各预测模型进行训练。所提出的DM-MDN模型由三层神经网络构成主干，后续的四个分支输出模型结构均为单层神经网络，并全部使用ReLU作为激活函数。为了评估性能，我们还选取了两种对比模型：基于GRU的模型利用PyTorch标准GRU结构堆叠4层，后接两层神经网络作为输出层；而基于Transformer的模型在编码器部分采用PyTorch提供的TransformerEncoderLayer，并在输出层设置单层神经网络。
###### 网络拓扑和流量模型
我们构建了由单个接入点(AP)和多个客户端节点组成的星型网络拓扑，节点的数量取决于当前的实验场景，数量为3-7个。采用单跳传输方式进行数据包传输。实验中各节点均直接与AP相连，无需中继节点。
为模拟真实环境中的动态流量特性，我们为每个节点设计了基于三元组参数的随机流量模型。该模型由offset(起始偏移时间，0-3000微秒)、packet_interval(数据包到达间隔，500-2500微秒)及arrive_probability(数据包到达概率，0.7-0.99)组成，这些参数均通过均匀分布的随机算法生成。

##### 预测实验结果
在预测信道空闲长度的任务中，主要对比的基线模型为GRU和Transformer，这两个对比模型的具体结构在 模型结构 这一节中已经详细介绍。
###### 预测精度结果
表 1 展示了 DM-MDN 和 MDN 相较于基线模型在测试数据集上的预测绝对误差对比。测试数据集涵盖了不同网络负载情况下的仿真数据，且网络负载的变化是完全随机的。需要特别说明的是，在该测试中，DM-MDN 的最终预测值 final_mu 计算方式为 mu + mu_bias，其原因在于本实验旨在验证混合密度网络（MDN）能否准确预测当前网络的负载情况。而 mu_bias 的去除通常是为了避免预测值偏大，从而提升模型的在网络场景下的性能。
由于网络环境具有高度随机性，四种方法的预测精度均相对较低。然而，混合密度网络能够有效捕捉训练数据中的多峰分布特性，使其能够更精准地判断网络的当前负载情况，并基于负载状态进行合理预测。从表 1 可见，在预测精度方面，混合密度网络较基线模型 GRU 和 Transformer 分别提升了约 xx% 和 xx%，显示出其在复杂网络环境下的优越性。

###### 预测模型在仿真中的结果
我们在三种不同负载场景下，对比了DM-MDN与基线方法在实际网络仿真中的性能表现。表1、表2和表3分别展示了平均数据包时延、丢包率和系统吞吐量的对比结果。

xxxx（具体表现）

在低负载场景下，我们发现基于GRU和Transformer的预测方法，在仿真中的结果和DM-MDN相比出现较大的差异。通过分析在仿真中GRU和Transformer的预测值发现，它们无法捕捉到当前是低负载的场景，其输出的预测值依然集中在高负载场景下的信道空闲情况。这导致了GRU和Transformer的预测值较小，而当节点数据包到达时，数据包到达的时隙很难落在预测的信道空闲时隙，这导致了节点数据包经常错过了发送时机。

##### LLM-Agent 实验结果
我们对比了本文提出的LLM Agent资源分配框架与Reflexion、Strategy以及传统贪心算法在四种大语言模型上的性能表现以及它们的资源占用情况。
###### 动态场景
为全面评估本文提出的LLM Agent资源分配框架性能，我们构建了一个具有动态优先级特性的无线网络实验场景。该场景由五个普通节点和一个接入点(AP)组成，网络中每个节点的数据包优先级均呈现动态变化特性。数据包优先级分为三个等级：优先级1（最低）、优先级2（中等）和优先级3（最高），其中优先级1的数据包不受传输时限限制，而优先级2和优先级3的数据包则分别设置了1200微秒和900微秒的截止时限(deadline)约束。
在实验过程中，每个节点的数据包优先级由均匀分布的随机算法确定，模拟实际网络中业务需求的变化特性。当节点数据包优先级发生变化后，系统会将网络中所有节点的最新优先级信息提交至资源分配算法。该算法根据当前网络状态和各节点需求，决定每个节点应采用的信道接入策略（传统CSMA/CA或基于DM-MDN的预测方法），并为选择DM-MDN策略的节点确定最优分位数参数，从而实现对异构节点的差异化资源分配与管理。
考虑到本研究主要针对动态实时网络环境，我们将高优先级节点的丢包率作为主要评估指标。在具有严格时限要求的场景中，数据包的可靠传输对保障服务质量至关重要，特别是对优先级较高的业务而言，丢包率能够直接反映算法在异构资源需求管理方面的能力。基于这一考虑，我们在LLM Agent的设计中将降低高优先级数据包的丢包率设定为优化的核心目标，以验证所提出框架在动态环境下的适应性和决策能力。
###### LLM-Agent 实验结果分析

###### 算法资源开销分析