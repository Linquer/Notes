#### Experiment Results
本章节将评估DM-MDN预测的准确性验证和其实际性能，其次，对比不同LLM Agents方法的性能及其在资源占用方面的差异。本实验基于一个开源的IEEE 802.11g网络仿真平台展开，该平台由Micheletto等人开发。

##### Experimental Setup
###### Simulation platform
在本研究中，我们构建的仿真环境基于IEEE 802.11g标准，采用2.4 GHz频段，传输速率为54 Mbit/s，竞争窗口设置为CW_MIN = 16和CW_MAX = 1024；在信号传播方面，则采用自由空间路径损耗（FSPL, Free Space Path Loss）模型来描述信号功率随距离的衰减情况，从而更真实地反映无线传输的物理特性。
网络拓扑为星型拓扑，由单个接入点和3至7个客户端节点组成，节点数量根据实验场景需求进行调整。在Dynamic Traffic Model 章节中提到的三元组参数流量模型。以下为三个参数均匀分布采样的范围：offset(0,3000us)：起始偏移时间，interval(500,2500us):数据包到达间隔，prob(0.7,0.99):数据包到达概率。对于数据包优先级的deadline做以下设置：优先级1的数据包不做deadline限制，优先级2的数据包deadline为1200us，优先级3的数据包deadline为900us
###### 模型结构
预测实验采用了基于70000个数据包仿真获取的信道空闲数据作为训练数据。DM-MDN模型由三层神经网络构成主干，配合四个单层神经网络分支作为输出结构，全部使用ReLU激活函数。为进行性能评估，我们选择了两种对比模型：一种基于GRU，由PyTorch标准GRU结构堆叠4层并接两层神经网络输出层；另一种基于Transformer，其编码器采用PyTorch提供的TransformerEncoderLayer堆叠2层，输出层为单层神经网络。


##### 预测实验结果
###### 预测精度结果
DM-MDN的本质是对网络负载进行识别并给出在当前负载下信道空闲时隙长度的区间。在无线网络中，信道空闲时隙长度具有高度随机性，因此DM-MDN在特定均值和标准差下的高斯采样，其预测值会有很大的误差。然而，DM-MDN的优势在于其能够准确识别网络负载状态，并据此动态调整采样区间。正是基于这一特性，DM-MDN在预测精度上相比GRU和Transformer模型展现出显著优势。
表 1 展示了 DM-MDN 和 MDN 在随机采样下相较于对比模型在测试数据集上的预测Mean Absolute Error(MAE)对比结果。测试数据集包含了不同网络负载条件下的数据，并且网络负载变化是完全随机特性。需要特别说明的是，在该实验中，DM-MDN 的均值计算公式为 μ = μ_base + μ_offset，这主要是因为我们旨在验证 DM-MDN 的预测精度而不是实际表现。从表 1 数据可以看出，DM-MDN 在预测精度方面较 GRU 和 Transformer 分别领先了约 xx% 和 xx%，展现了其在复杂网络环境下的优势。此外，DM-MDN 与 MDN 在预测精度上差异极小，这侧面证明了将均值进行结构化分解并不会损害模型的整体预测能力。

###### 预测模型在仿真中的结果
我们在三种不同负载场景和三种流量模型下，比较了DM-MDN、Transformer、GRU和CSMA/CA在网络仿真中的性能表现。表2展示了这四种方法在数据包平均时延、丢包率和吞吐量方面的对比结果。三个流量模型的参数分别为：Flow1：(2222，2107，0.96)，Flow2：(2741, 637，0.79)，Flow3：(930，1188，0.81)。其中，Flow1的数据包到达周期最长，但其数据包到达概率最高；Flow2的数据包到达周期最短，但数据包到达概率最大；Flow3则介于两者之间。我们特意选取了这三个具有鲜明特点的流量模型，目的是为了更加全面地评估各方法在不同场景下的性能表现。
在该实验中，MD-MDN、Transformer和GRU这三组实验，采用预测算法的节点数据包优先级为3，其他节点数据包优先级为1采用CSMA/CA接入算法。对于CSMA/CA实验，网络中所有节点均采用CSMA/CA接入算法，但是采用特定FLow的节点数据包优先级为3，其他节点数据包优先级为1。三种负载情况对于的发送数据包节点个数为：3(Low)、5(Medium)、7(High)。

xxxx（具体表现）
根据表2的实验结果，我们可以观察到两个明显的现象。首先，CSMA/CA算法的性能随着网络负载的增加而下降，特别是在网络负载较高的情况下，其性能明显退化，三个指标的表现均明显落后于其他方法。其次，基于GRU和Transformer的预测算法在低负载条件下表现出显著的性能退化，这一现象与直觉相反，通常在低负载下，信道接入方法的性能应该是最优的。通过分析实验数据，我们发现这两种方法未能准确识别低负载环境，预测值仍然偏向高负载时的信道空闲状态。因此，它们的预测值较小，导致节点数据包在预测的空闲时隙内到达的概率较低，从而错失了最佳发送时机，这一情况与Fig 3中的场景3相符。相较之下，DM-MDN算法能够准确识别当前网络负载情况，并及时调整预测值的范围，因此在低负载情况下表现稳定，没有出现明显的性能退化。总体来看，虽然DM-MDN算法并未在所有场景下都优于其他对比方法，但凭借其对网络多峰负载特性的有效识别，使得其综合性能在所有方法中表现最佳且最为稳定。


###### 不同分位数下的算法表现
对于DM-MDN而言，不同的分位数参数代表了不同的信道接入能力。理论上，分位数越高信道接入能力相对越强。但是由于网络场景的复杂性，最优分位数参数会随网络环境变化而不同。
图3展示了在三种不同场景下，采用DM-MDN节点在不同分位数下的数据包丢包率。实验环境包含6个节点，其中图3的(a)、(b)、(c)子图分别对应1个、2个、3个节点采用DM-MDN作为信道接入算法的情况（采用DM-MDN的节点数据包优先级均为3）。图3(d)则综合展示了这三种场景下DM-MDN节点的平均数据丢包率，以及与采用CSMA/CA且数据包优先级为3的节点丢包率进行了对比。
从图3可以看出，DM-MDN算法的丢包率总体上随着分位数的增加而减少。然而，最优分位数在不同场景下各不相同，且并都不是最大值0.9。因此，确定特定场景下的最优分位数，需要综合考虑历史信息和当前网络环境特征。值得注意的是，如图3(d)所示，虽然随着采用DM-MDN的节点数量增多，丢包率也随之增加，但无论采用何种分位数，DM-MDN算法的丢包率整体上都优于CSMA/CA算法的节点。


##### LLM-Agent 实验结果
###### 场景设计
为全面评估NetCogAgents资源分配表现，设计了一个具有动态优先级特性的无线网络场景。该场景包含5个节点和1个接入点(AP)，每个节点的数据包优先级动态变化。在时限要求严格的场景中，数据包的可靠传输至关重要，本研究聚焦动态实时网络环境，因此采用高优先级节点的丢包率作为主要评估指标。
实验中，部分节点的数据包优先级通过均匀分布的随机算法确定，以模拟节点业务需求变化。节点优先级变化时，系统会更新并提交优先级信息至资源分配算法，后者根据网络状态和节点需求选择合适的信道接入策略（CSMA/CA或DM-MDN），并为DM-MDN设置分位数参数，实现差异化资源分配。从图3可以看出，当网络中采用DM-MDN的节点达到总节点数一半时，其丢包率相较于其他场景有大幅波动。为了更加准确测试出资源分配算法对于网络场景变化的敏感度，在该实验中，采用高优先级的节点不会超过总节点数的一半。

###### LLM-Agent 实验结果分析
我们对NetCogAgents与Reflexion、StrategyLLM以及传统的贪心+自适应方法进行了性能对比。对于LLM Agent方法，分别在四种LLMs上进行测试，并在每种LLMs进行了三轮节点动态需求变化测试。表3展示了这四种方法在实验中高优先级节点的平均数据包丢包率。在这十二次实验中，每次实验节点的数据包优先级变化序列均不相同。
实验结果表明，NetCogAgents在平均丢包率方面优于其他方法，表现相对稳定，并没有出现丢包率非常高的情况。这主要归功于其高效的经验组织方式和反思机制所实现的快速适应能力。
Reflexion的性能波动最为显著，既有优异表现也有明显劣势。这种不稳定性主要源于其保留所有决策轨迹的机制，随着系统持续运行，决策依据逐渐变得冗余复杂，导致其决策模块难以从大量历史轨迹中提取有效信息。
StrategyLLM展现出较为稳定的性能，但其决策质量在很大程度上取决于初始生成策略的质量。在正确策略指导下，StrategyLLM能够接近最优决策；然而，若初始策略不当，其决策效果则显著降低。此外，StrategyLLM的策略更新机制较为缓慢，难以有效应对快速变化的动态场景需求。
传统方法主要采用贪心与自适应相结合的策略。具体而言，对高优先级节点应用DM-MDN方法，而低优先级节点则采用CSMA/CA方法。DM-MDN的分位数初始值设定为0.5，并通过自适应策略根据每次的丢包率结果以0.1为步长进行上下调整，以实现性能优化。这种贪心机制能让该方法在初期取得相对好的成绩，但是其策略调整较为机械，后续策略调整的速度明显不如其他方法。

###### 内存资源开销分析
考虑到网络管理设备的存储空间通常是有限的，同时，过长的决策历史输入也可能影响大型语言模型的推理效率与决策质量。因此，在设计NetCogAgents结构的时候就考虑到，需要在保证决策效果的前提下，尽可能地减少决策历史记录所占用的内存。图X展示了NetCogAgents、Reflexion和StrategyLLM在四种大语言模型中，在网络决策的过程中它们占用内存的情况。

在此需要说明，图中的"Memory Usage"并非指LLMs输出的实际内存占用情况。由于大语言模型的输出长度在不同的网络场景下会发生变化，即使在相同语境下也可能存在显著差异。因此，采用了更便于量化的统一计量方式：对于NetCogAgents，每条经验记录计为1单位内存占用；对于Reflexion，每次历史决策轨迹及其对应的反思经验共同计为1单位内存占用；对于StrategyLLM，每条初始生成的策略计为1单位内存占用。

从图X中可以明显观察到Reflexion随着决策轮次增长内存占用也持续上升，这是由于其每轮决策都会保留完整的轨迹并生成新的反思内容。相比之下，StrategyLLM仅在系统初始化时生成预设的M条策略，后续若有策略更新也会直接覆盖原有策略，因此其内存占用始终保持稳定。NetCogAgents的内存使用则呈现出小范围波动但整体较低的趋势，这主要得益于反思智能体持续优化并精简其经验区内容。并且，经验智能体在面对相同场景时通常不会生成新的决策内容，除非决策智能体更新了策略并取得更优的结果。整体上，在反思智能体和特定的经验区管理规则共同作用下，NetCogAgents在内存占用表现上，均领先对比方法。
