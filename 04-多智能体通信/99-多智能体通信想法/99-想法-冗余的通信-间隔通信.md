先进行一个实验，在t时刻，随机选择在t时刻之前的信息进行发送，查看效果

就是在一个固定时间范围内的消息进行打乱，看看效果是否会变差很多

如果效果不会变差很多，则说明了多智能体的通信很多时候是冗余的

- 因此我们做一种算法，如何判断自身消息的新颖性，来确定是否发送消息

#### 几个实现方向
##### 集中式
- 有一个中央的集中信息融合器，集中式训练，执行时下发到各个智能体
- 每个Agent根据自身消息的特性（例如新颖性、重要性）来确定是否向中央信息融合器更新自己的消息
- 每个Agent根据某种指标（需要考虑），来确定是否向中央信息融合器拉取消息

##### 特定的代表未来的意图消息
- 在证明通信存在冗余的情况下，为减少通信，一种思路是让通信的消息更具有未来的使用价值
- 寻找一种方法实现通信消息代表该Agent未来的意图、观察趋势
- 在特定的阈值下，进行共享消息
- 核心点：如何让消息可以让别的Agent用得更久
- 关于定制通信内容可以参考： 
  - 09: 通过熵和互信息来训练通信的内容
  - 10：中对通信消息进行方差限制
  - 13：CoDe，让消息表示未来意图
  - 15：训练一个自编码器，让中间向量作为信息通信的内容
  - 17：提供了两种：让消息还原为全局状态、对比学习
  - 16和18：分享的不是消息而是激励。16：分享的是Q偏置，17：分享的是奖励reward


##### 如何确定是否发送消息的间隔
- 是否发送消息的判断依据可以参考：
  - 03：
  - 06: 将带宽约束引入到是否发送消息决策
  - 07: 判断消息共享和不共享前后动作的Q的差值作为是否共享信息的训练依据
- 关于判断其他智能体消息是否有益方法：
  - 07：Gated-ACML 通过计算有消息和无消息两个Q的差值
  - 14：通过通信和不通信前后输出决策的不确定性的差值进行确定，可以再详细阅读一下
- 如何衡量通信的信息增益

##### 消息与期限
- 生成消息的模型每次输出不仅仅包含消息，还包括描述这个消息的有效期限
- 有效期限的使用
  - 含义：代表这个消息在未来的多少步就会过期
  - 使用：在其他agent进行消息融合的时候，需要参考这个过期时限。每个时刻之后，对期限进行减一操作
    - RNN：可以根据或者期限排列该消息的序列，期限越大的，越后面经过RNN，这样可以保留更多消息
    - Attention：可以将期限作为特征输入到Attention中，也可以用期限对 qk 出来的 a 进行调整
    - 规则相加：根据消息的期限进行规则相加
  - 解读：
    - 在间隔通信中，或者是非完全通信中，智能体间的消息不一定都是当前时刻传来的。
    - 因此在信息融合时需要考虑消息的时效性，如果消息较为老旧，可以适当降低其消息融合时的权重。
    - 但是，这不能仅仅根据消息生成/到达之后的时间进行确定权重，因为有些消息可能非常具有未来价值，可以多时刻重复利用。
    - 因此，在生成消息之初，可以附带生成该消息的有效期限，以此让其他·agent更好的利用已收到的消息。
- 有效期限的训练
  - 消息生成器输出采用双头机制，输出包括消息和期限(d)
  - 期限输出头采用监督学习
  - 通过对比m_t和m_t+i(i 可以大于d也可以小于d)，什么时候这个消息的差距大于某个阈值时，就说明需要重新发送消息
  - 这个 i 就是期限 d 监督学习的 y
- 消息的定制
- 如何判断消息之间的差距
- 故事
  - 更加有根据的消息融合
  - 基于消息本身的间隔通信


#### 间隔通信----消息反馈
- 概念
  - 在agent进行消息融合的时候，会生成一个权重，将这个权重反馈给发送该消息的agent
  - agent就能够知道该消息在其他人那里的重要性，因此可以将这个反馈作为自己发送消息频率的依据
- 间隔通信
  - 方案1：将权重反馈作为奖励
    - 采用两个新的RL作为训练消息发送的方法
    - 输入为智能体某个隐藏层的输出，输入还可以添加其他信息来做定制化消息
    - 隐藏层输入Message_net层，输出为消息m
    - 消息的定制就可以专门训练Message_net
    - 消息m再经过一个Gate_net层，Gate_net输出一个标志位，该标志位用于判断是否发送消息
    - Gate_net训练：
      - 其他智能体对该智能体消息的反馈记为 a
      - a 是每个智能体归一化之后的权重
      - 若Gate_net选择不发送消息，则给予 1/(n-1) 的奖励，n为智能体数量
      - 若Gate_net选择发送消息，则给予 Σ a_i/(n-1) 的奖励
      - 以此来鼓励Gate_net学习出发送有价值的消息
    - Message_net训练：
       - 输入：待确定
       - 输出：消息m
       - 对于获得Gate_net为1的消息，给予正奖励
       - 对于获得Gate_net为0的消息，给予负奖励
       - 以此来鼓励Message_net学习生成有价值的消息


#### 请求与拉去


#### 需要考虑的点
- 对比其他Gate那种方法，创新点是什么
  - 和之前的方法通过限制通信，这种方法通过可解释的方式进行
  - 明确是否通信的标准，而不是通过一个Gate输出来决定
  - 06批判：只是考虑环境带宽，并没有参考智能体信息是否新颖，每考虑到智能体是否迫切需要更新消息
  - 07批判：消息并不代表未来意图，说明一下Q值的缺点
  


#### Ask to LLM
- 在多智能体强化学习领域，众多研究表明，通过多智能体间的通信能够显著提升效果。但是目前多智能体通信有两个需要考虑的点，1、网络带宽有限，很多时候无法支撑多智能体之间的全通信。2、多智能体之间的通信存在冗余，很多通信的消息是没有必要的，甚至会影响智能体策略的学习。因此我考虑两个点做一种多智能体强化学习通信的方法，1、消息表示未来意图，为了减少多智能体通信的次数，因此训练一个消息发送器，让通信的消息更加代表未来意图。2、间隔通信，由于消息更多代表未来意图，因此不需要全通信，对于如何间隔，我目前的想法是根据t和t-i时刻消息增益进行判断，当前这个增益如何确定目前还没有确定的想法。现在请你进行头脑风暴，帮我想想我这两个点应该进行如何设计，才能成为一篇高水平论文的idea。
- 关于对消息的定制方面，目前的我调研的内容如下，1、IMAC这个方法，为了减少通信内容，对输出的通信内容m采用互信息进行限制，将m与一个简单分布的进行互信息限制，以此达到降低信息熵。2、VBC这个方法，对通信消息进行方差限制，控制通信信息的方差，如果发现信息的方差较大，则说明该信息较为有意义。3、CoDe，这篇文章将自己的历史观察h编码为信息m，通过监督学习，希望从m中能够提取出后t步的动作预测，以此实现通信的内容代表未来意图，这个方法和你的较为类似。4、AE-Comm，这个方法训练一个自编码器，大致是o->m->o，就是让通信的m能够准确表达出当前智能体的观测o。5、ExpoComm，这个方法希望智能体的通信消息m能够还原出全局观测，以实现智能体分享更多有意义的信息。6、还有一些方法分享的不是传统的消息，而是分享奖励，或者Q偏置，直接影响其他智能体决策。现在我们只专注讨论定制化消息这个问题，请问有哪些新颖的想法，也不一定要局限于消息一定要表示未来意图，也可以是其他定制化手段，让这个消息能够支撑间隔通信。请你提供更多的想法，不要只给一两个。



#### 过去的消息的如何融合

#### 这个消息如何代表未来的意图

#### 消息发送间隔确定