##### 整体想法
我目前做的是多智能体强化学习并采用通信的方法增强协作能力。整体的背景是在网络受限的环境下进行，因此需要减少通信的次数、考虑网络质量。
我现在的想法可以分为3部分，第一就是网络预算，第二个是更具反馈的消息定制，第三个就是确定消息的间隔。
第一部分网络预算。首先每个智能体通过探测包计算出一些网络环境的指标，比如丢包率、延迟、抖动等，然后根据这些指标来确定智能体的网络预算。网络预算是指智能体在N个决策时刻，可以发送多少M个消息，M <= N。我现在的基本想法是通过ns-3构建一个模拟环境，然后通过探测包来计算网络环境指标，然后在计算后面T时间长度整个网络环境一共发送了多少个数据包。采用这种预测和监督学习的方式，来训练网络预算模块。当然还需要将T时间转变为多少个决策时刻，以及数据包的数量转化为多少个消息。
第二部分基于其他智能体反馈的消息定制。我目前暂定每个智能体在融合消息的时候会对每个消息生成一个权重，例如这个权重可以拿注意力机制的qk所生成的a来确定。然后每个智能体需要对所有消息的权重做一个softmax，然后将这个权重广播出去。因此，每轮决策之后，每个智能体都会收到其他智能体的反馈。每个智能体可以从广播帧中提取到自己消息的反馈，再将接收到的n-1个反馈加起来再除以n-1，得到一个平均的权重。这个平均权重的平均值为1/(n-1)。对于消息定制模块也是一个强化学习模块。输入的s为原始消息（可以是每个智能体某层的输出），输出a为通信消息，奖励就是该消息拿到的反馈，如果这个反馈大于1/(n-1)，则奖励为1，否则为-1。这样理论上就能训练出更符合其他智能体需求的消息。
第三部分确定消息的间隔。这个模块可以命名为门控模块，门控模块主要解决是否发送消息以及如何避免冲突（网络领域称为碰撞）。对于这个模块输入实现这些功能目前我的定义还不是很明确。首先它要实现的功能是避免冲突，由于每个智能体都会获取到其他智能体的反馈，并且反馈的消息是广播的，其他智能体也能够收到。因此可以计算出目前得分最高的智能体，所以下一时刻最优先能够发送数据包的智能体就是目前得分最高的智能体。但是目前这种发送机制缺乏公平性和探索性。其次，这也没考虑到网络预算的问题。

现在我需要你全面评估这个想法，看看在大方向上有什么问题，有什么需要修改的。第二，请你详细说说网络预算模块如何使用ns-3进行收集数据和训练。第三，你觉得这个消息定制模块的实现思路。第四，请你详细说说消息间隔部分门控模块如何综合考虑反馈和网络预算，实现一种分布式的消息发送策略。

#### 待优化问题
- 反馈包丢失问题
- 网络预算的训练数据加入真实网络数据
- 分阶段学习：
  - 阶段一：先不考虑网络限制，重点训练消息定制模块，让智能体先学会生成“有价值”的消息。
  - 阶段二：引入网络预算和门控模块，在“学会协作”的基础上，学习“高效节约地协作”。
- 广播的消息反馈是否能够整合进其他数据包一起发送，或者降低消息反馈数据包的消耗
- ## 在门控计算是否发送的时候，可以计算上一次发送消息和这一次发送消息之间的差距（KL散度）以及发送间隔
  - 如果两个消息差距较大说明应该发送
  - 如果间隔较大，也说明需要更新
- 消息定制模块的奖励需要加入全局任务的奖励


#### 反馈包丢失问题
- 在消息定制模块，会顺带训练一个消息评估模块
- 消息评估模块能够评估消息的得分
- 如果反馈包丢失的话，就拿消息评估模块的评分作为消息的得分

#### 不需要反馈包的情况设计
- 消息评估模块可以直接计算出每个消息的得分
- 直接按照消息评估模块的得分来确定是否发送消息

#### 门控模块





我需要你在r_mappo/algorithm/r_actor_critic.py加一种MARL通信算法。首先我需要构建一个MARL通信消息融合模块。消息融合发生在主干网络rnn之后act之前。首先我将rnn的输出actor_features作为消息融合模块的输入之一，消息融合还需要包含其他num_agent-1个消息,注意num_agent由我提供，你只需要作为该模块的参数即可，不需要操作它怎么来的。每个消息的维度是一样的。消息融合模块主要采用attention算法，但是我的attention不太一样。首先actor_features经过一个线性层作其输出作为全局的Q，然后其他num_agent-1个消息分别进入两个线性层，输出分别作为K和V。然后Q和每个K通过一个线性层输出一个值a。最后所有的V乘上自己K和全局Q算出的a，最后全部加起来。
注意，你只需要完成消息融合模块的定义，以及在R_Actor类中的定义以及使用。最后actor_features拼接上消息融合模块的输出，作为act网络的输入